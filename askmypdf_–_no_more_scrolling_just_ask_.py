# -*- coding: utf-8 -*-
"""AskMyPDF â€“ No more scrolling. Just ask.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYf26jpunvpeIPaTnoEbnhDc5kEaqWvN

**AskMyPDF â€“ No more scrolling. Just ask.**
"""

# STEP 1: Install required packages
!pip install -q cassio langchain langchain-community langchain-huggingface datasets sentence-transformers pdfplumber

# STEP 2: Import libraries
import os
import pdfplumber
import cassio
import uuid

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Cassandra
from langchain.indexes.vectorstore import VectorStoreIndexWrapper
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEndpoint

# STEP 3: Set credentials
os.environ["ASTRA_DB_APPLICATION_TOKEN"] = "INSERT YOUR ASTRA DB TOKEN"
os.environ["ASTRA_DB_ID"] = "INSERT YOUR ASTRA DB ID"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "INSERT YOUR HUGGING-FACE TOKEN"


cassio.init(
    token=os.environ["ASTRA_DB_APPLICATION_TOKEN"],
    database_id=os.environ["ASTRA_DB_ID"]
)

# STEP 4: Function to load and process any PDF
def process_pdf(pdf_path):
    print(f"ðŸ“„ Loading PDF: {pdf_path}")

    # Extract text using pdfplumber
    raw_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                raw_text += text + "\n"

    # Split into chunks
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    texts = text_splitter.split_text(raw_text)

    print(f"âœ… Extracted {len(texts)} chunks from the PDF.")

    return texts

# STEP 5: Function to initialize vector store with a fresh PDF
def create_vector_store(texts):
    unique_table_name = f"pdf_chat_{str(uuid.uuid4())[:8]}"  # new table for each PDF

    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Cassandra(
        embedding=embedding,
        table_name=unique_table_name,
        session=None,
        keyspace=None,
    )

    vector_store.add_texts(texts)
    print(f"âœ… Inserted {len(texts)} chunks into vector store: {unique_table_name}")

    return vector_store

# STEP 6: Load Zephyr LLM from Hugging Face
llm = HuggingFaceEndpoint(
    repo_id="HuggingFaceH4/zephyr-7b-beta",
    huggingfacehub_api_token=os.environ["HUGGINGFACEHUB_API_TOKEN"],
    temperature=0.3,
    task="text-generation"
)

# STEP 7: Ask questions with intelligent prompts
def ask_questions(vector_store):
    vector_index = VectorStoreIndexWrapper(vectorstore=vector_store)

    while True:
        query = input("\nðŸ”Ž Enter your question (or type 'quit'): ").strip()
        if query.lower() == "quit":
            break
        if not query:
            continue

        documents = vector_store.similarity_search(query, k=5)
        context = "\n".join([doc.page_content for doc in documents])

        prompt = f"""You are a helpful assistant. Use the context below to answer the user's question.
If the answer is not present, say "Not found in the document." Don't guess or include unrelated info.

Context:
{context}

Question: {query}
Answer:"""

        answer = llm.invoke(prompt)
        print(f"\nâœ… ANSWER: {answer.strip()}\n")
        print("ðŸ“„ CONTEXT USED:")
        for doc in documents:
            print(f"   â€¢ {doc.page_content[:100].strip()}...")

# STEP 8: Upload and run (manually input path in Colab or use upload widget)
pdf_path = "Enter your file path"  # Change this to your uploaded file path
texts = process_pdf(pdf_path)
store = create_vector_store(texts)
ask_questions(store)